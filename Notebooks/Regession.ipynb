{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Modèle mathématique\n",
    "La régression linéaire multiple modélise la relation entre une variable dépendante $y$ et plusieurs variables indépendantes $x_1, x_2, \\dots, x_n$. Le modèle mathématique s'écrit comme suit :\n",
    "$$\n",
    "y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\dots + w_n \\cdot x_n + b\n",
    "$$\n",
    "En notation matricielle :\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\cdot \\mathbf{w} + \\mathbf{b}\n",
    "$$\n",
    "où :\n",
    "\n",
    "$\\mathbf{X}$ : Matrice $m \\times n$ (m : nombre d'échantillons, n : nombre de variables indépendantes)\n",
    "$\\mathbf{w}$ : Vecteur colonne contenant les poids $[w_1, w_2, \\dots, w_n]^\\top$\n",
    "$\\mathbf{b}$ : Vecteur ou scalaire représentant le biais\n",
    "\n",
    "# 2 Fonction de coût : Erreur quadratique moyenne (MSE)\n",
    "La fonction de coût mesure l'erreur entre les prédictions $\\hat{\\mathbf{y}}$ et les valeurs réelles $\\mathbf{y}$ :\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right)^2\n",
    "$$\n",
    "où :\n",
    "\n",
    "$\\hat{y}_i = \\mathbf{w}^\\top \\cdot \\mathbf{x}_i + b$\n",
    "$m$ : Nombre d'échantillons\n",
    "\n",
    "# 3 Descente de gradient\n",
    "La descente de gradient optimise les paramètres $\\mathbf{w}$ et $b$ pour minimiser $J$.\n",
    "Mise à jour des poids $\\mathbf{w}$ :\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} - \\alpha \\cdot \\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "$$\n",
    "Mise à jour du biais $b$ :\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "où $\\alpha$ est le taux d'apprentissage.\n",
    "# 4 Calcul des gradients\n",
    "Gradient par rapport aux poids $\\mathbf{w}$ :\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{m} \\cdot \\mathbf{X}^\\top \\cdot \\left( \\hat{\\mathbf{y}} - \\mathbf{y} \\right)\n",
    "$$\n",
    "Gradient par rapport au biais $b$ :\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\cdot \\sum_{i=1}^{m} \\left( \\hat{y}_i - y_i \\right)\n",
    "$$\n",
    "# 5 Résolution analytique (Optionnelle)\n",
    "La minimisation de $J$ peut également être résolue analytiquement (sans descente de gradient) avec la formule fermée suivante :\n",
    "$$\n",
    "\\mathbf{w} = \\left( \\mathbf{X}^\\top \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "Cette approche nécessite que $\\mathbf{X}^\\top \\mathbf{X}$ soit inversible.\n",
    "# 6 Interprétation des coefficients\n",
    "\n",
    "Chaque coefficient $w_j$ représente l'effet marginal de $x_j$ sur $y$, en supposant que les autres variables restent constantes\n",
    "Le biais $b$ est la valeur de $y$ lorsque toutes les variables $x_1, x_2, \\dots, x_n$ valent zéro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
